[
  {
    "title": "Visual-RFT: Visual Reinforcement Fine-Tuning",
    "summary": "Reinforcement Fine-Tuning (RFT) in Large Reasoning Models like OpenAI o1\nlearns from feedback on its answers, which is especially useful in applications\nwhen fine-tuning data is scarce. Recent open-source work like DeepSeek-R1\ndemonstrates that reinforcement learning with verifiable reward is one key\ndirection in reproducing o1. While the R1-style model has demonstrated success\nin language models, its application in multi-modal domains remains\nunder-explored. This work introduces Visual Reinforcement Fine-Tuning\n(Visual-RFT), which further extends the application areas of RFT on visual\ntasks. Specifically, Visual-RFT first uses Large Vision-Language Models (LVLMs)\nto generate multiple responses containing reasoning tokens and final answers\nfor each input, and then uses our proposed visual perception verifiable reward\nfunctions to update the model via the policy optimization algorithm such as\nGroup Relative Policy Optimization (GRPO). We design different verifiable\nreward functions for different perception tasks, such as the Intersection over\nUnion (IoU) reward for object detection. Experimental results on fine-grained\nimage classification, few-shot object detection, reasoning grounding, as well\nas open-vocabulary object detection benchmarks show the competitive performance\nand advanced generalization ability of Visual-RFT compared with Supervised\nFine-tuning (SFT). For example, Visual-RFT improves accuracy by 24.3% over\nthe baseline in one-shot fine-grained image classification with around 100\nsamples. In few-shot object detection, Visual-RFT also exceeds the baseline by\n21.9 on COCO's two-shot setting and 15.4 on LVIS. Our Visual-RFT represents\na paradigm shift in fine-tuning LVLMs, offering a data-efficient, reward-driven\napproach that enhances reasoning and adaptability for domain-specific tasks.",
    "translation": "标题：Visual-RFT：视觉强化微调\n\n摘要：在大型推理模型（如OpenAI o1）中，强化微调（Reinforcement Fine-Tuning, RFT）通过对其答案的反馈进行学习，这在微调数据稀缺的应用中尤为有用。最近的开源工作（如DeepSeek-R1）表明，具有可验证奖励的强化学习是复现o1的关键方向之一。尽管R1风格的模型在语言模型中已取得成功，但其在多模态领域的应用仍未被充分探索。本文提出了视觉强化微调（Visual-RFT），进一步扩展了RFT在视觉任务中的应用领域。具体而言，Visual-RFT首先使用大型视觉语言模型（Large Vision-Language Models, LVLMs）为每个输入生成包含推理标记和最终答案的多个响应，然后使用我们提出的视觉感知可验证奖励函数，通过策略优化算法（如组相对策略优化，Group Relative Policy Optimization, GRPO）更新模型。我们为不同的感知任务设计了不同的可验证奖励函数，例如用于目标检测的交并比（Intersection over Union, IoU）奖励。在细粒度图像分类、少样本目标检测、推理定位以及开放词汇目标检测基准上的实验结果表明，与监督微调（Supervised Fine-tuning, SFT）相比，Visual-RFT具有竞争性的性能和更强的泛化能力。例如，在仅有约100个样本的单样本细粒度图像分类任务中，Visual-RFT将准确率比基线提高了24.3%。在少样本目标检测任务中，Visual-RFT在COCO的双样本设置上比基线高出21.9，在LVIS上高出15.4。我们的Visual-RFT代表了微调LVLMs的范式转变，提供了一种数据高效、奖励驱动的方法，增强了领域特定任务的推理能力和适应性。",
    "url": "https://huggingface.co/papers/2503.01785",
    "arxiv_url": "https://arxiv.org/abs/2503.01785"
  },
  {
    "title": "Difix3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "summary": "Neural Radiance Fields and 3D Gaussian Splatting have revolutionized 3D\nreconstruction and novel-view synthesis task. However, achieving photorealistic\nrendering from extreme novel viewpoints remains challenging, as artifacts\npersist across representations. In this work, we introduce Difix3D+, a novel\npipeline designed to enhance 3D reconstruction and novel-view synthesis through\nsingle-step diffusion models. At the core of our approach is Difix, a\nsingle-step image diffusion model trained to enhance and remove artifacts in\nrendered novel views caused by underconstrained regions of the 3D\nrepresentation. Difix serves two critical roles in our pipeline. First, it is\nused during the reconstruction phase to clean up pseudo-training views that are\nrendered from the reconstruction and then distilled back into 3D. This greatly\nenhances underconstrained regions and improves the overall 3D representation\nquality. More importantly, Difix also acts as a neural enhancer during\ninference, effectively removing residual artifacts arising from imperfect 3D\nsupervision and the limited capacity of current reconstruction models. Difix3D+\nis a general solution, a single model compatible with both NeRF and 3DGS\nrepresentations, and it achieves an average 2times improvement in FID score\nover baselines while maintaining 3D consistency.",
    "translation": "标题：Difix3D+：通过单步扩散模型改进三维重建\n\n摘要：神经辐射场（Neural Radiance Fields）和三维高斯溅射（3D Gaussian Splatting）技术已经彻底改变了三维重建和新视角合成任务。然而，从极端新视角实现逼真渲染仍然具有挑战性，因为伪影在各种表示中持续存在。在本研究中，我们提出了Difix3D+，一种旨在通过单步扩散模型增强三维重建和新视角合成的新颖流程。我们方法的核心是Difix，这是一种单步图像扩散模型，经过训练以增强和消除由三维表示中未充分约束区域引起的渲染新视角中的伪影。Difix在我们的流程中扮演两个关键角色。首先，它在重建阶段用于清理从重建中渲染的伪训练视图，然后将其蒸馏回三维。这极大地增强了未充分约束的区域，并提高了整体三维表示的质量。更重要的是，Difix还在推理过程中充当神经增强器，有效消除由于不完善的三维监督和当前重建模型能力有限而产生的残余伪影。Difix3D+是一种通用解决方案，一个与NeRF和3DGS表示兼容的单一模型，它在保持三维一致性的同时，将FID评分平均提高了2倍于基线水平。",
    "url": "https://huggingface.co/papers/2503.01774",
    "arxiv_url": "https://arxiv.org/abs/2503.01774"
  },
  {
    "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language\n  Models via Mixture-of-LoRAs",
    "summary": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable\nlanguage and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language\nmodel trained on high-quality web and synthetic data, significantly\noutperforming recent open-source models of similar size and matching the\nperformance of models twice its size on math and coding tasks requiring complex\nreasoning. This achievement is driven by a carefully curated synthetic data\nrecipe emphasizing high-quality math and coding datasets. Compared to its\npredecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of\n200K tokens to better support multilingual applications, as well as group query\nattention for more efficient long-sequence generation. Phi-4-Multimodal is a\nmultimodal model that integrates text, vision, and speech/audio input\nmodalities into a single model. Its novel modality extension approach leverages\nLoRA adapters and modality-specific routers to allow multiple inference modes\ncombining various modalities without interference. For example, it now ranks\nfirst in the OpenASR leaderboard to date, although the LoRA component of the\nspeech/audio modality has just 460 million parameters. Phi-4-Multimodal\nsupports scenarios involving (vision + language), (vision + speech), and\n(speech/audio) inputs, outperforming larger vision-language and speech-language\nmodels on a wide range of tasks. Additionally, we experiment to further train\nPhi-4-Mini to enhance its reasoning capabilities. Despite its compact\n3.8-billion-parameter size, this experimental version achieves reasoning\nperformance on par with or surpassing significantly larger models, including\nDeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.",
    "translation": "标题：Phi-4-Mini技术报告：通过混合LoRAs实现紧凑而强大的多模态语言模型\n\n摘要：我们介绍了Phi-4-Mini和Phi-4-Multimodal，这是两款紧凑但能力极强的语言和多模态模型。Phi-4-Mini是一个拥有38亿参数的语言模型，基于高质量的网络和合成数据进行训练，在需要复杂推理的数学和编程任务上，显著优于近期开源的类似规模模型，并匹敌规模是其两倍的模型。这一成就得益于精心设计的合成数据配方，特别强调高质量的数学和编程数据集。与上一代Phi-3.5-Mini相比，Phi-4-Mini的词汇量扩展至20万个标记，以更好地支持多语言应用，并采用组查询注意力机制以提高长序列生成的效率。Phi-4-Multimodal是一款多模态模型，将文本、视觉和语音/音频输入模式集成于单一模型中。其新颖的模式扩展方法利用LoRA适配器和特定模式的路由器，允许多种模式组合的推理模式互不干扰。例如，尽管语音/音频模式的LoRA组件仅有4.6亿参数，它已在OpenASR排行榜上位居首位。Phi-4-Multimodal支持涉及（视觉+语言）、（视觉+语音）和（语音/音频）输入的场景，在多种任务上超越更大的视觉-语言和语音-语言模型。此外，我们通过实验进一步训练Phi-4-Mini以增强其推理能力。尽管其紧凑的38亿参数规模，这一实验版本在推理性能上达到或超越了包括DeepSeek-R1-Distill-Qwen-7B和DeepSeek-R1-Distill-Llama-8B在内的显著更大的模型。",
    "url": "https://huggingface.co/papers/2503.01743",
    "arxiv_url": "https://arxiv.org/abs/2503.01743"
  },
  {
    "title": "OneRec: Unifying Retrieve and Rank with Generative Recommender and\n  Iterative Preference Alignment",
    "summary": "Recently, generative retrieval-based recommendation systems have emerged as a\npromising paradigm. However, most modern recommender systems adopt a\nretrieve-and-rank strategy, where the generative model functions only as a\nselector during the retrieval stage. In this paper, we propose OneRec, which\nreplaces the cascaded learning framework with a unified generative model. To\nthe best of our knowledge, this is the first end-to-end generative model that\nsignificantly surpasses current complex and well-designed recommender systems\nin real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder\nstructure, which encodes the user's historical behavior sequences and gradually\ndecodes the videos that the user may be interested in. We adopt sparse\nMixture-of-Experts (MoE) to scale model capacity without proportionally\nincreasing computational FLOPs. 2) a session-wise generation approach. In\ncontrast to traditional next-item prediction, we propose a session-wise\ngeneration, which is more elegant and contextually coherent than point-by-point\ngeneration that relies on hand-crafted rules to properly combine the generated\nresults. 3) an Iterative Preference Alignment module combined with Direct\nPreference Optimization (DPO) to enhance the quality of the generated results.\nUnlike DPO in NLP, a recommendation system typically has only one opportunity\nto display results for each user's browsing request, making it impossible to\nobtain positive and negative samples simultaneously. To address this\nlimitation, We design a reward model to simulate user generation and customize\nthe sampling strategy. Extensive experiments have demonstrated that a limited\nnumber of DPO samples can align user interest preferences and significantly\nimprove the quality of generated results. We deployed OneRec in the main scene\nof Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial\nimprovement.",
    "translation": "标题：OneRec：基于生成式推荐和迭代偏好对齐的统一检索与排序模型\n\n摘要：近年来，基于生成式检索的推荐系统已成为一种有前景的范式。然而，大多数现代推荐系统采用检索-排序策略，其中生成模型仅在检索阶段作为选择器使用。本文提出了OneRec，它用一个统一的生成模型取代了级联学习框架。据我们所知，这是第一个在现实场景中显著超越当前复杂且精心设计的推荐系统的端到端生成模型。具体而言，OneRec包括：1）一个编码器-解码器结构，该结构编码用户的历史行为序列，并逐步解码用户可能感兴趣的视频。我们采用稀疏专家混合（MoE）来扩展模型容量，而不会按比例增加计算量。2）一种会话式生成方法。与传统的下一项预测不同，我们提出了一种会话式生成方法，它比依赖手工规则来正确组合生成结果的逐点生成方法更为优雅且上下文连贯。3）一个结合直接偏好优化（DPO）的迭代偏好对齐模块，以提高生成结果的质量。与自然语言处理中的DPO不同，推荐系统通常只有一次机会为每个用户的浏览请求显示结果，因此无法同时获得正负样本。为了解决这一限制，我们设计了一个奖励模型来模拟用户生成并定制采样策略。大量实验表明，有限数量的DPO样本可以对齐用户兴趣偏好，并显著提高生成结果的质量。我们在快手的主场景中部署了OneRec，实现了1.6%的观看时间增长，这是一个显著的改进。",
    "url": "https://huggingface.co/papers/2502.18965",
    "arxiv_url": "https://arxiv.org/abs/2502.18965"
  },
  {
    "title": "When an LLM is apprehensive about its answers -- and when its\n  uncertainty is justified",
    "summary": "Uncertainty estimation is crucial for evaluating Large Language Models\n(LLMs), particularly in high-stakes domains where incorrect answers result in\nsignificant consequences. Numerous approaches consider this problem, while\nfocusing on a specific type of uncertainty, ignoring others. We investigate\nwhat estimates, specifically token-wise entropy and model-as-judge (MASJ),\nwould work for multiple-choice question-answering tasks for different question\ntopics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of\ndifferent sizes from 1.5B to 72B and 14 topics. While MASJ performs similarly\nto a random error predictor, the response entropy predicts model error in\nknowledge-dependent domains and serves as an effective indicator of question\ndifficulty: for biology ROC AUC is 0.73. This correlation vanishes for the\nreasoning-dependent domain: for math questions ROC-AUC is 0.55. More\nprincipally, we found out that the entropy measure required a reasoning amount.\nThus, data-uncertainty related entropy should be integrated within uncertainty\nestimates frameworks, while MASJ requires refinement. Moreover, existing\nMMLU-Pro samples are biased, and should balance required amount of reasoning\nfor different subdomains to provide a more fair assessment of LLMs performance.",
    "translation": "标题：当大型语言模型对其答案感到担忧时——以及其不确定性何时是合理的\n\n摘要：不确定性估计对于评估大型语言模型（LLMs）至关重要，特别是在高风险领域，错误的答案会导致严重后果。许多方法在考虑这一问题时，专注于特定类型的不确定性，而忽略了其他类型。我们研究了哪些估计方法，特别是基于词元的熵和模型作为评判者（MASJ），适用于不同主题的多项选择题回答任务。我们的实验考虑了三种不同规模的LLMs：Phi-4、Mistral和Qwen，规模从1.5B到72B，涵盖了14个主题。虽然MASJ的表现类似于随机错误预测器，但响应熵在知识依赖领域中预测模型错误，并作为问题难度的有效指标：对于生物学，ROC AUC为0.73。这种相关性在推理依赖领域中消失：对于数学问题，ROC-AUC为0.55。更根本的是，我们发现熵测量需要一定的推理量。因此，与数据不确定性相关的熵应整合到不确定性估计框架中，而MASJ则需要改进。此外，现有的MMLU-Pro样本存在偏差，应平衡不同子领域所需的推理量，以提供更公平的LLMs性能评估。",
    "url": "https://huggingface.co/papers/2503.01688",
    "arxiv_url": "https://arxiv.org/abs/2503.01688"
  },
  {
    "title": "DiffRhythm: Blazingly Fast and Embarrassingly Simple End-to-End\n  Full-Length Song Generation with Latent Diffusion",
    "summary": "Recent advancements in music generation have garnered significant attention,\nyet existing approaches face critical limitations. Some current generative\nmodels can only synthesize either the vocal track or the accompaniment track.\nWhile some models can generate combined vocal and accompaniment, they typically\nrely on meticulously designed multi-stage cascading architectures and intricate\ndata pipelines, hindering scalability. Additionally, most systems are\nrestricted to generating short musical segments rather than full-length songs.\nFurthermore, widely used language model-based methods suffer from slow\ninference speeds. To address these challenges, we propose DiffRhythm, the first\nlatent diffusion-based song generation model capable of synthesizing complete\nsongs with both vocal and accompaniment for durations of up to 4m45s in only\nten seconds, maintaining high musicality and intelligibility. Despite its\nremarkable capabilities, DiffRhythm is designed to be simple and elegant: it\neliminates the need for complex data preparation, employs a straightforward\nmodel structure, and requires only lyrics and a style prompt during inference.\nAdditionally, its non-autoregressive structure ensures fast inference speeds.\nThis simplicity guarantees the scalability of DiffRhythm. Moreover, we release\nthe complete training code along with the pre-trained model on large-scale data\nto promote reproducibility and further research.",
    "translation": "标题：DiffRhythm：基于潜在扩散的极速且极简端到端全长歌曲生成方法\n\n摘要：近年来，音乐生成领域取得了显著进展，然而现有方法仍面临一些关键限制。当前的一些生成模型只能合成人声轨道或伴奏轨道。虽然有些模型能够生成包含人声和伴奏的完整音乐，但它们通常依赖于精心设计的多阶段级联架构和复杂的数据处理流程，这限制了其可扩展性。此外，大多数系统仅限于生成短音乐片段而非全长歌曲。同时，广泛使用的基于语言模型的方法存在推理速度慢的问题。为解决这些挑战，我们提出了DiffRhythm，这是首个基于潜在扩散的歌曲生成模型，能够在仅十秒内合成包含人声和伴奏的完整歌曲，时长可达4分45秒，同时保持较高的音乐性和可理解性。尽管DiffRhythm具有卓越的性能，但其设计简洁优雅：它无需复杂的数据准备，采用简单的模型结构，在推理过程中仅需歌词和风格提示。此外，其非自回归结构确保了快速的推理速度。这种简洁性保证了DiffRhythm的可扩展性。我们还发布了完整的训练代码以及在大规模数据上预训练的模型，以促进研究的可重复性和进一步探索。",
    "url": "https://huggingface.co/papers/2503.01183",
    "arxiv_url": "https://arxiv.org/abs/2503.01183"
  },
  {
    "title": "Liger: Linearizing Large Language Models to Gated Recurrent Structures",
    "summary": "Transformers with linear recurrent modeling offer linear-time training and\nconstant-memory inference. Despite their demonstrated efficiency and\nperformance, pretraining such non-standard architectures from scratch remains\ncostly and risky. The linearization of large language models (LLMs) transforms\npretrained standard models into linear recurrent structures, enabling more\nefficient deployment. However, current linearization methods typically\nintroduce additional feature map modules that require extensive fine-tuning and\noverlook the gating mechanisms used in state-of-the-art linear recurrent\nmodels. To address these issues, this paper presents Liger, short for\nLinearizing LLMs to gated recurrent structures. Liger is a novel approach for\nconverting pretrained LLMs into gated linear recurrent models without adding\nextra parameters. It repurposes the pretrained key matrix weights to construct\ndiverse gating mechanisms, facilitating the formation of various gated\nrecurrent structures while avoiding the need to train additional components\nfrom scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA),\nLiger restores the performance of the linearized gated recurrent models to\nmatch that of the original LLMs. Additionally, we introduce Liger Attention, an\nintra-layer hybrid attention mechanism, which significantly recovers 93\\% of\nthe Transformer-based LLM at 0.02\\% pre-training tokens during the\nlinearization process, achieving competitive results across multiple\nbenchmarks, as validated on models ranging from 1B to 8B parameters. Code is\navailable at https://github.com/OpenSparseLLMs/Linearization.",
    "translation": "标题：Liger：将大型语言模型线性化为门控循环结构\n\n摘要：具有线性循环建模的Transformer提供了线性时间训练和恒定内存推理。尽管它们展示了效率和性能，但从头开始预训练这种非标准架构仍然成本高昂且风险较大。大型语言模型（LLM）的线性化将预训练的标准模型转化为线性循环结构，从而实现更高效的部署。然而，当前的线性化方法通常引入额外的特征映射模块，这些模块需要大量的微调，并且忽略了最先进的线性循环模型中使用的门控机制。为了解决这些问题，本文提出了Liger，即“将LLM线性化为门控循环结构”的简称。Liger是一种将预训练的LLM转换为门控线性循环模型的新方法，无需添加额外参数。它重新利用预训练的键矩阵权重来构建多样化的门控机制，促进各种门控循环结构的形成，同时避免了从头训练额外组件的需求。通过使用低秩适应（LoRA）进行轻量级微调，Liger恢复了线性化门控循环模型的性能，使其与原始LLM相匹配。此外，我们引入了Liger Attention，一种层内混合注意力机制，在仅使用0.02%的预训练令牌的情况下，显著恢复了基于Transformer的LLM的93%性能，在多个基准测试中取得了具有竞争力的结果，这在1B到8B参数的模型上得到了验证。代码可在https://github.com/OpenSparseLLMs/Linearization获取。",
    "url": "https://huggingface.co/papers/2503.01496",
    "arxiv_url": "https://arxiv.org/abs/2503.01496"
  },
  {
    "title": "Qilin: A Multimodal Information Retrieval Dataset with APP-level User\n  Sessions",
    "summary": "User-generated content (UGC) communities, especially those featuring\nmultimodal content, improve user experiences by integrating visual and textual\ninformation into results (or items). The challenge of improving user\nexperiences in complex systems with search and recommendation (S\\&R) services\nhas drawn significant attention from both academia and industry these years.\nHowever, the lack of high-quality datasets has limited the research progress on\nmultimodal S\\&R. To address the growing need for developing better S\\&R\nservices, we present a novel multimodal information retrieval dataset in this\npaper, namely Qilin. The dataset is collected from Xiaohongshu, a popular\nsocial platform with over 300 million monthly active users and an average\nsearch penetration rate of over 70\\%. In contrast to existing datasets,\nQilin offers a comprehensive collection of user sessions with\nheterogeneous results like image-text notes, video notes, commercial notes, and\ndirect answers, facilitating the development of advanced multimodal neural\nretrieval models across diverse task settings. To better model user\nsatisfaction and support the analysis of heterogeneous user behaviors, we also\ncollect extensive APP-level contextual signals and genuine user feedback.\nNotably, Qilin contains user-favored answers and their referred results for\nsearch requests triggering the Deep Query Answering (DQA) module. This allows\nnot only the training \\& evaluation of a Retrieval-augmented Generation (RAG)\npipeline, but also the exploration of how such a module would affect users'\nsearch behavior. Through comprehensive analysis and experiments, we provide\ninteresting findings and insights for further improving S\\&R systems. We hope\nthat Qilin will significantly contribute to the advancement of\nmultimodal content platforms with S\\&R services in the future.",
    "translation": "标题：Qilin：一个包含APP级用户会话的多模态信息检索数据集\n\n摘要：用户生成内容（UGC）社区，尤其是那些以多模态内容为特色的社区，通过将视觉和文本信息整合到结果（或项目）中来提升用户体验。近年来，在复杂系统中提升搜索和推荐（S&R）服务的用户体验的挑战引起了学术界和工业界的广泛关注。然而，高质量数据集的缺乏限制了对多模态S&R的研究进展。为了满足开发更好S&R服务的日益增长的需求，本文提出了一个新颖的多模态信息检索数据集，即Qilin。该数据集收集自小红书，这是一个拥有超过3亿月活跃用户且平均搜索渗透率超过70%的流行社交平台。与现有数据集相比，Qilin提供了全面的用户会话集合，包含图像-文本笔记、视频笔记、商业笔记和直接答案等异构结果，促进了跨多种任务设置的高级多模态神经检索模型的开发。为了更好地建模用户满意度并支持异构用户行为的分析，我们还收集了广泛的APP级上下文信号和真实的用户反馈。值得注意的是，Qilin包含了用户偏好的答案及其在触发深度查询回答（DQA）模块的搜索请求中引用的结果。这不仅允许训练和评估检索增强生成（RAG）管道，还允许探索此类模块如何影响用户的搜索行为。通过全面的分析和实验，我们为进一步改进S&R系统提供了有趣的发现和见解。我们希望Qilin将显著推动未来具有S&R服务的多模态内容平台的发展。",
    "url": "https://huggingface.co/papers/2503.00501",
    "arxiv_url": "https://arxiv.org/abs/2503.00501"
  },
  {
    "title": "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four\n  Habits of Highly Effective STaRs",
    "summary": "Test-time inference has emerged as a powerful paradigm for enabling language\nmodels to ``think'' longer and more carefully about complex challenges, much\nlike skilled human experts. While reinforcement learning (RL) can drive\nself-improvement in language models on verifiable tasks, some models exhibit\nsubstantial gains while others quickly plateau. For instance, we find that\nQwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game\nof Countdown. This discrepancy raises a critical question: what intrinsic\nproperties enable effective self-improvement? We introduce a framework to\ninvestigate this question by analyzing four key cognitive behaviors --\nverification, backtracking, subgoal setting, and backward chaining -- that both\nexpert human problem solvers and successful language models employ. Our study\nreveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama\ninitially lacks them. In systematic experimentation with controlled behavioral\ndatasets, we find that priming Llama with examples containing these reasoning\nbehaviors enables substantial improvements during RL, matching or exceeding\nQwen's performance. Importantly, the presence of reasoning behaviors, rather\nthan correctness of answers, proves to be the critical factor -- models primed\nwith incorrect solutions containing proper reasoning patterns achieve\ncomparable performance to those trained on correct solutions. Finally,\nleveraging continued pretraining with OpenWebMath data, filtered to amplify\nreasoning behaviors, enables the Llama model to match Qwen's self-improvement\ntrajectory. Our findings establish a fundamental relationship between initial\nreasoning behaviors and the capacity for improvement, explaining why some\nlanguage models effectively utilize additional computation while others\nplateau.",
    "translation": "标题：实现自我改进推理者的认知行为，或高效STaRs的四大习惯\n\n摘要：测试时推理已成为一种强大的范式，使语言模型能够像熟练的人类专家一样，对复杂挑战进行更长时间和更深入的“思考”。虽然强化学习（RL）可以在可验证任务中推动语言模型的自我改进，但一些模型表现出显著的提升，而其他模型则迅速达到瓶颈。例如，我们发现，在相同的RL训练下，Qwen-2.5-3B在Countdown游戏中的表现远远超过Llama-3.2-3B。这种差异引发了一个关键问题：哪些内在属性能够实现有效的自我改进？我们引入了一个框架，通过分析四种关键认知行为——验证、回溯、子目标设定和反向链——来研究这个问题，这些行为既是人类专家问题解决者也是成功的语言模型所采用的。我们的研究表明，Qwen自然表现出这些推理行为，而Llama最初缺乏这些行为。在控制行为数据集的系统实验中，我们发现，通过在RL训练前为Llama提供包含这些推理行为的示例，可以显著提升其表现，达到或超过Qwen的表现。重要的是，推理行为的存在，而非答案的正确性，被证明是关键因素——使用包含正确推理模式的错误解决方案进行训练的模型，其表现与使用正确解决方案训练的模型相当。最后，利用OpenWebMath数据进行持续预训练，并通过过滤增强推理行为，使Llama模型能够匹配Qwen的自我改进轨迹。我们的研究结果确立了初始推理行为与改进能力之间的基本关系，解释了为什么一些语言模型能够有效利用额外的计算资源，而其他模型则停滞不前。",
    "url": "https://huggingface.co/papers/2503.01307",
    "arxiv_url": "https://arxiv.org/abs/2503.01307"
  },
  {
    "title": "Speculative Ad-hoc Querying",
    "summary": "Analyzing large datasets requires responsive query execution, but executing\nSQL queries on massive datasets can be slow. This paper explores whether query\nexecution can begin even before the user has finished typing, allowing results\nto appear almost instantly. We propose SpeQL, a system that leverages Large\nLanguage Models (LLMs) to predict likely queries based on the database schema,\nthe user's past queries, and their incomplete query. Since exact query\nprediction is infeasible, SpeQL speculates on partial queries in two ways: 1)\nit predicts the query structure to compile and plan queries in advance, and 2)\nit precomputes smaller temporary tables that are much smaller than the original\ndatabase, but are still predicted to contain all information necessary to\nanswer the user's final query. Additionally, SpeQL continuously displays\nresults for speculated queries and subqueries in real time, aiding exploratory\nanalysis. A utility/user study showed that SpeQL improved task completion time,\nand participants reported that its speculative display of results helped them\ndiscover patterns in the data more quickly. In the study, SpeQL improves user's\nquery latency by up to 289times and kept the overhead reasonable, at 4$\nper hour.",
    "translation": "标题：推测性即时查询\n\n摘要：分析大型数据集需要响应迅速的查询执行，但在海量数据集上执行SQL查询可能会很慢。本文探讨了是否可以在用户完成输入之前就开始查询执行，从而使结果几乎即时显示。我们提出了SpeQL系统，该系统利用大型语言模型（LLMs）根据数据库模式、用户过去的查询及其不完整的查询来预测可能的查询。由于精确的查询预测是不可行的，SpeQL通过两种方式对部分查询进行推测：1）预测查询结构以提前编译和计划查询，2）预计算比原始数据库小得多的临时表，但仍预测包含回答用户最终查询所需的所有信息。此外，SpeQL实时连续显示推测查询和子查询的结果，有助于探索性分析。一项效用/用户研究表明，SpeQL提高了任务完成时间，参与者报告称其推测性结果显示帮助他们更快地发现数据中的模式。在该研究中，SpeQL将用户的查询延迟提高了最多289倍，并将开销保持在合理水平，每小时4美元。",
    "url": "https://huggingface.co/papers/2503.00714",
    "arxiv_url": "https://arxiv.org/abs/2503.00714"
  },
  {
    "title": "DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with\n  Dynamic Multi-Sequence Drafting",
    "summary": "Large language models (LLMs) exhibit exceptional performance across a wide\nrange of tasks; however, their token-by-token autoregressive generation process\nsignificantly hinders inference speed. Speculative decoding presents a\npromising draft-then-verify framework that reduces generation latency while\nmaintaining output distribution fidelity. Nevertheless, the draft model\nintroduces additional computational overhead, becoming a performance bottleneck\nand increasing the time to first token (TTFT). Previous approaches to mitigate\ndraft model overhead have primarily relied on heuristics and generally failed\nto match the quality of the draft language models. To address these challenges,\nwe propose DuoDecoding, a novel approach that strategically deploys the draft\nand target models on the CPU and GPU respectively, enabling parallel decoding\nwhile preserving draft quality. Our method incorporates a hardware-aware\noptimal draft budget to minimize idle times and employs dynamic multi-sequence\ndrafting to enhance draft quality. Extensive experiments across seven tasks\nshow that DuoDecoding achieves up to 2.61x speedup in generation latency, while\nreducing TTFT to 83% of that in conventional speculative decoding. The Code is\navailable at https://github.com/KaiLv69/DuoDecoding.",
    "translation": "标题：DuoDecoding：基于硬件感知的异构推测解码与动态多序列草稿生成\n\n摘要：大语言模型（LLMs）在广泛的任务中表现出卓越的性能；然而，其逐令牌的自回归生成过程显著阻碍了推理速度。推测解码提供了一种有前景的草稿-验证框架，能够在保持输出分布保真度的同时减少生成延迟。然而，草稿模型引入了额外的计算开销，成为性能瓶颈并增加了首令牌生成时间（TTFT）。以往减轻草稿模型开销的方法主要依赖于启发式方法，通常无法匹配草稿语言模型的质量。为了解决这些挑战，我们提出了DuoDecoding，这是一种新颖的方法，策略性地将草稿模型和目标模型分别部署在CPU和GPU上，从而实现并行解码并保持草稿质量。我们的方法结合了硬件感知的最优草稿预算，以最小化空闲时间，并采用动态多序列草稿生成来提升草稿质量。在七个任务上的广泛实验表明，DuoDecoding在生成延迟上实现了高达2.61倍的加速，同时将TTFT降低到传统推测解码的83%。代码可在https://github.com/KaiLv69/DuoDecoding获取。",
    "url": "https://huggingface.co/papers/2503.00784",
    "arxiv_url": "https://arxiv.org/abs/2503.00784"
  },
  {
    "title": "SampleMix: A Sample-wise Pre-training Data Mixing Strategey by\n  Coordinating Data Quality and Diversity",
    "summary": "Existing pretraining data mixing methods for large language models (LLMs)\ntypically follow a domain-wise methodology, a top-down process that first\ndetermines domain weights and then performs uniform data sampling across each\ndomain. However, these approaches neglect significant inter-domain overlaps and\ncommonalities, failing to control the global diversity of the constructed\ntraining dataset. Further, uniform sampling within domains ignores fine-grained\nsample-specific features, potentially leading to suboptimal data distribution.\nTo address these shortcomings, we propose a novel sample-wise data mixture\napproach based on a bottom-up paradigm. This method performs global\ncross-domain sampling by systematically evaluating the quality and diversity of\neach sample, thereby dynamically determining the optimal domain distribution.\nComprehensive experiments across multiple downstream tasks and perplexity\nassessments demonstrate that SampleMix surpasses existing domain-based methods.\nMeanwhile, SampleMix requires 1.4x to 2.1x training steps to achieves the\nbaselines' performance, highlighting the substantial potential of SampleMix to\noptimize pre-training data.",
    "translation": "标题：SampleMix：一种通过协调数据质量和多样性的样本级预训练数据混合策略\n\n摘要：现有的大型语言模型（LLMs）预训练数据混合方法通常遵循一种领域级的方法论，这是一种自上而下的过程，首先确定领域权重，然后在每个领域内进行统一的数据采样。然而，这些方法忽视了显著的领域间重叠和共性，未能控制构建的训练数据集的全局多样性。此外，领域内的统一采样忽略了细粒度的样本特定特征，可能导致次优的数据分布。为了解决这些不足，我们提出了一种基于自下而上范式的新型样本级数据混合方法。该方法通过系统评估每个样本的质量和多样性，进行全局跨领域采样，从而动态确定最优的领域分布。在多个下游任务和困惑度评估中的综合实验表明，SampleMix超越了现有的基于领域的方法。同时，SampleMix需要1.4倍到2.1倍的训练步骤才能达到基线性能，这凸显了SampleMix在优化预训练数据方面的巨大潜力。",
    "url": "https://huggingface.co/papers/2503.01506",
    "arxiv_url": "https://arxiv.org/abs/2503.01506"
  },
  {
    "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
    "summary": "Diffusion models have achieved great success in generating 2D images.\nHowever, the quality and generalizability of 3D content generation remain\nlimited. State-of-the-art methods often require large-scale 3D assets for\ntraining, which are challenging to collect. In this work, we introduce\nKiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient\nframework for generating, editing, and enhancing 3D objects by repurposing a\nwell-trained 2D image diffusion model for 3D generation. Specifically, we\nfine-tune a diffusion model to generate ''3D Bundle Image'', a tiled\nrepresentation composed of multi-view images and their corresponding normal\nmaps. The normal maps are then used to reconstruct a 3D mesh, and the\nmulti-view images provide texture mapping, resulting in a complete 3D model.\nThis simple method effectively transforms the 3D generation problem into a 2D\nimage generation task, maximizing the utilization of knowledge in pretrained\ndiffusion models. Furthermore, we demonstrate that our Kiss3DGen model is\ncompatible with various diffusion model techniques, enabling advanced features\nsuch as 3D editing, mesh and texture enhancement, etc. Through extensive\nexperiments, we demonstrate the effectiveness of our approach, showcasing its\nability to produce high-quality 3D models efficiently.",
    "translation": "标题：Kiss3DGen：将图像扩散模型重新用于3D资产生成\n\n摘要：扩散模型在生成2D图像方面取得了巨大成功。然而，3D内容生成的质量和通用性仍然有限。最先进的方法通常需要大规模的3D资产进行训练，这些资产的收集具有挑战性。在本研究中，我们引入了Kiss3DGen（在3D生成中保持简单和直接），这是一个通过重新利用训练良好的2D图像扩散模型进行3D生成的高效框架，用于生成、编辑和增强3D对象。具体来说，我们对扩散模型进行微调，以生成“3D捆绑图像”，这是一种由多视角图像及其对应的法线图组成的平铺表示。然后，法线图用于重建3D网格，多视角图像提供纹理映射，从而生成完整的3D模型。这种简单的方法有效地将3D生成问题转化为2D图像生成任务，最大限度地利用了预训练扩散模型中的知识。此外，我们展示了Kiss3DGen模型与各种扩散模型技术的兼容性，能够实现3D编辑、网格和纹理增强等高级功能。通过大量实验，我们证明了该方法的有效性，展示了其高效生成高质量3D模型的能力。",
    "url": "https://huggingface.co/papers/2503.01370",
    "arxiv_url": "https://arxiv.org/abs/2503.01370"
  },
  {
    "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
    "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
    "translation": "标题：从小时到分钟：无损加速超长序列生成至10万标记\n\n摘要：使用大型语言模型（LLMs）生成超长序列变得越来越重要，但仍然是一项非常耗时的任务，特别是对于长达10万标记的序列。虽然存在传统的推测解码方法，但简单地扩展其生成限制并不能加速这一过程，反而可能有害。通过深入分析，我们识别出阻碍高效生成的三大挑战：频繁的模型重载、动态键值（KV）管理和重复生成。为了解决这些问题，我们引入了TOKENSWIFT，这是一个新颖的框架，旨在显著加速超长序列的生成过程，同时保持目标模型的固有质量。实验结果表明，TOKENSWIFT在不同规模（1.5B、7B、8B、14B）和架构（MHA、GQA）的模型中实现了超过3倍的加速。这一加速为超长序列生成节省了数小时的时间，使TOKENSWIFT成为前所未有的长度上可扩展且有效的解决方案。代码可在https://github.com/bigai-nlco/TokenSwift找到。",
    "url": "https://huggingface.co/papers/2502.18890",
    "arxiv_url": "https://arxiv.org/abs/2502.18890"
  },
  {
    "title": "Large-Scale Data Selection for Instruction Tuning",
    "summary": "Selecting high-quality training data from a larger pool is a crucial step\nwhen instruction-tuning language models, as carefully curated datasets often\nproduce models that outperform those trained on much larger, noisier datasets.\nAutomated data selection approaches for instruction-tuning are typically tested\nby selecting small datasets (roughly 10k samples) from small pools (100-200k\nsamples). However, popular deployed instruction-tuned models often train on\nhundreds of thousands to millions of samples, subsampled from even larger data\npools. We present a systematic study of how well data selection methods scale\nto these settings, selecting up to 2.5M samples from pools of up to 5.8M\nsamples and evaluating across 7 diverse tasks. We show that many recently\nproposed methods fall short of random selection in this setting (while using\nmore compute), and even decline in performance when given access to larger\npools of data to select over. However, we find that a variant of\nrepresentation-based data selection (RDS+), which uses weighted mean pooling of\npretrained LM hidden states, consistently outperforms more complex methods\nacross all settings tested -- all whilst being more compute-efficient. Our\nfindings highlight that the scaling properties of proposed automated selection\nmethods should be more closely examined. We release our code, data, and models\nat https://github.com/hamishivi/automated-instruction-selection.",
    "translation": "标题：大规模数据选择用于指令调优\n\n摘要：从更大的数据池中选择高质量的训练数据是指令调优语言模型的关键步骤，因为精心策划的数据集通常能产生优于在更大、噪声更多的数据集上训练的模型。用于指令调优的自动化数据选择方法通常通过从小型数据池（大约10万到20万样本）中选择小型数据集（大约1万样本）进行测试。然而，流行的已部署指令调优模型通常训练在数十万到数百万样本上，这些样本是从更大的数据池中抽取的。我们对数据选择方法在这些设置中的扩展能力进行了系统研究，从最多580万样本的数据池中选择最多250万样本，并在7个不同的任务上进行评估。我们发现，许多最近提出的方法在这种设置下表现不如随机选择（同时使用更多的计算资源），甚至在可以选择更大数据池时性能下降。然而，我们发现一种基于表示的数据选择变体（RDS+），它使用预训练语言模型隐藏状态的加权平均池化，在所有测试设置中始终优于更复杂的方法——同时计算效率更高。我们的研究结果强调，应更密切地检查所提出的自动化选择方法的扩展特性。我们在https://github.com/hamishivi/automated-instruction-selection发布了我们的代码、数据和模型。",
    "url": "https://huggingface.co/papers/2503.01807",
    "arxiv_url": "https://arxiv.org/abs/2503.01807"
  },
  {
    "title": "Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia",
    "summary": "Human readers can efficiently comprehend scrambled words, a phenomenon known\nas Typoglycemia, primarily by relying on word form; if word form alone is\ninsufficient, they further utilize contextual cues for interpretation. While\nadvanced large language models (LLMs) exhibit similar abilities, the underlying\nmechanisms remain unclear. To investigate this, we conduct controlled\nexperiments to analyze the roles of word form and contextual information in\nsemantic reconstruction and examine LLM attention patterns. Specifically, we\nfirst propose SemRecScore, a reliable metric to quantify the degree of semantic\nreconstruction, and validate its effectiveness. Using this metric, we study how\nword form and contextual information influence LLMs' semantic reconstruction\nability, identifying word form as the core factor in this process. Furthermore,\nwe analyze how LLMs utilize word form and find that they rely on specialized\nattention heads to extract and process word form information, with this\nmechanism remaining stable across varying levels of word scrambling. This\ndistinction between LLMs' fixed attention patterns primarily focused on word\nform and human readers' adaptive strategy in balancing word form and contextual\ninformation provides insights into enhancing LLM performance by incorporating\nhuman-like, context-aware mechanisms.",
    "translation": "标题：词形至关重要：大语言模型在Typoglycemia下的语义重建\n\n摘要：人类读者能够有效地理解被打乱的单词，这种现象被称为Typoglycemia，主要依赖于词形；如果仅凭词形不足以理解，他们还会进一步利用上下文线索进行解释。虽然先进的大语言模型（LLMs）表现出类似的能力，但其背后的机制尚不明确。为了研究这一点，我们进行了控制实验，以分析词形和上下文信息在语义重建中的作用，并检查LLM的注意力模式。具体来说，我们首先提出了SemRecScore，这是一个可靠的指标，用于量化语义重建的程度，并验证了其有效性。使用这一指标，我们研究了词形和上下文信息如何影响LLMs的语义重建能力，发现词形是这一过程中的核心因素。此外，我们分析了LLMs如何利用词形，发现它们依赖于专门的注意力头来提取和处理词形信息，这种机制在不同程度的单词打乱情况下保持稳定。LLMs主要关注词形的固定注意力模式与人类读者在平衡词形和上下文信息方面的自适应策略之间的区别，为通过融入类似人类的、上下文感知机制来提升LLM性能提供了见解。",
    "url": "https://huggingface.co/papers/2503.01714",
    "arxiv_url": "https://arxiv.org/abs/2503.01714"
  },
  {
    "title": "CodeArena: A Collective Evaluation Platform for LLM Code Generation",
    "summary": "Large Language Models (LLMs) have reshaped code generation by synergizing\ntheir exceptional comprehension of natural language and programming syntax,\nthereby substantially boosting developer productivity. These advancements have\nprompted numerous efforts to quantitatively evaluate their coding capabilities.\nHowever, persistent challenges, such as benchmark leakage, data dissipation,\nand limited system accessibility, continue to impede a timely and accurate\nassessment. To address these limitations, we introduce CodeArena, an online\nevaluation framework tailored for LLM code generation. The key innovation is a\ncollective evaluation mechanism, which dynamically recalibrates individual\nmodel scores based on the holistic performance of all participating models,\nmitigating score biases caused by widespread benchmark leakage. In addition,\nCodeArena ensures open access to all submitted solutions and test cases and\nprovides automation-friendly APIs to streamline the code evaluation workflow.\nOur main contributions are: (1) a collective evaluation system for unbiased\nassessment, (2) a public repository of solutions and test cases, and (3)\nautomation-ready APIs for seamless integration.",
    "translation": "标题：CodeArena：面向大语言模型代码生成的集体评估平台\n\n摘要：大语言模型（LLMs）通过融合其对自然语言和编程语法的卓越理解，重塑了代码生成领域，从而显著提升了开发者的生产力。这些进展促使了众多量化评估其编码能力的努力。然而，诸如基准泄露、数据分散和系统可访问性有限等持续存在的挑战，仍然阻碍了及时且准确的评估。为了解决这些限制，我们引入了CodeArena，一个专为LLM代码生成设计的在线评估框架。其核心创新在于一种集体评估机制，该机制根据所有参与模型的整体表现动态重新校准个体模型得分，从而减轻因广泛基准泄露导致的得分偏差。此外，CodeArena确保所有提交的解决方案和测试用例的公开访问，并提供自动化友好的API以简化代码评估工作流程。我们的主要贡献包括：（1）一个用于无偏评估的集体评估系统，（2）一个公开的解决方案和测试用例库，以及（3）支持自动化集成的API。",
    "url": "https://huggingface.co/papers/2503.01295",
    "arxiv_url": "https://arxiv.org/abs/2503.01295"
  },
  {
    "title": "PodAgent: A Comprehensive Framework for Podcast Generation",
    "summary": "Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.",
    "translation": "标题：PodAgent：一个全面的播客生成框架\n\n摘要：现有的自动音频生成方法难以有效地生成类似播客的音频节目。主要挑战在于深入的内容生成、适当且富有表现力的语音生成。本文提出了PodAgent，一个用于创建音频节目的综合框架。PodAgent 1) 通过设计一个主持人-嘉宾-作家多智能体协作系统生成信息丰富的主题讨论内容，2) 构建一个语音池以实现合适的语音角色匹配，3) 利用LLM增强的语音合成方法生成富有表现力的对话语音。鉴于缺乏类似播客音频生成的标准化评估标准，我们开发了全面的评估指南，以有效评估模型的性能。实验结果表明，PodAgent在主题讨论对话内容方面显著优于直接使用GPT-4生成的内容，语音匹配准确率达到87.4%，并通过LLM引导的合成生成更具表现力的语音。演示页面：https://podcast-agent.github.io/demo/。源代码：https://github.com/yujxx/PodAgent。",
    "url": "https://huggingface.co/papers/2503.00455",
    "arxiv_url": "https://arxiv.org/abs/2503.00455"
  },
  {
    "title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video\n  Generation",
    "summary": "Text-to-video generative models convert textual prompts into dynamic visual\ncontent, offering wide-ranging applications in film production, gaming, and\neducation. However, their real-world performance often falls short of user\nexpectations. One key reason is that these models have not been trained on\nvideos related to some topics users want to create. In this paper, we propose\nVideoUFO, the first Video dataset specifically curated to align with Users'\nFOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1)\nminimal (0.29%) overlap with existing video datasets, and (2) videos\nsearched exclusively via YouTube's official API under the Creative Commons\nlicense. These two attributes provide future researchers with greater freedom\nto broaden their training sources. The VideoUFO comprises over 1.09 million\nvideo clips, each paired with both a brief and a detailed caption\n(description). Specifically, through clustering, we first identify 1,291\nuser-focused topics from the million-scale real text-to-video prompt dataset,\nVidProM. Then, we use these topics to retrieve videos from YouTube, split the\nretrieved videos into clips, and generate both brief and detailed captions for\neach clip. After verifying the clips with specified topics, we are left with\nabout 1.09 million video clips. Our experiments reveal that (1) current 16\ntext-to-video models do not achieve consistent performance across all\nuser-focused topics; and (2) a simple model trained on VideoUFO outperforms\nothers on worst-performing topics. The dataset is publicly available at\nhttps://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0\nLicense.",
    "translation": "标题：VideoUFO：一个面向用户的百万规模文本到视频生成数据集\n\n摘要：文本到视频生成模型将文本提示转换为动态视觉内容，在电影制作、游戏和教育等领域具有广泛的应用前景。然而，这些模型在实际应用中的表现往往未能达到用户的期望。一个关键原因是这些模型并未针对用户希望创建的某些主题相关的视频进行训练。本文提出了VideoUFO，这是第一个专门为与现实场景中用户关注点对齐而策划的视频数据集。除此之外，VideoUFO还具有以下特点：（1）与现有视频数据集的重叠率极低（0.29%）；（2）所有视频均通过YouTube官方API在Creative Commons许可下搜索获取。这两个特点为未来的研究者提供了更大的自由度，以扩展其训练来源。VideoUFO包含超过109万个视频片段，每个片段都配有一个简短的标题和一个详细的描述（说明）。具体来说，通过聚类方法，我们首先从百万规模的真实文本到视频提示数据集VidProM中识别出1,291个用户关注的主题。然后，我们使用这些主题从YouTube上检索视频，将检索到的视频分割成片段，并为每个片段生成简短和详细的标题。在验证这些片段与指定主题的匹配性后，我们最终得到了约109万个视频片段。我们的实验表明：（1）当前的16个文本到视频模型在所有用户关注的主题上并未表现出一致的性能；（2）在VideoUFO上训练的简单模型在表现最差的主题上优于其他模型。该数据集在CC BY 4.0许可下公开于https://huggingface.co/datasets/WenhaoWang/VideoUFO。",
    "url": "https://huggingface.co/papers/2503.01739",
    "arxiv_url": "https://arxiv.org/abs/2503.01739"
  },
  {
    "title": "General Reasoning Requires Learning to Reason from the Get-go",
    "summary": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a\nreasoning prior for RL that can then be transferred to natural\nlanguage tasks, and (3) learning more generalizable reasoning functions using a\nsmall context window to reduce exploiting spurious correlations between tokens.\nSuch a reasoning system coupled with a trained retrieval system and a large\nexternal memory bank as a knowledge store can overcome several limitations of\nexisting architectures at learning to reason in novel scenarios.",
    "translation": "标题：通用推理需要从一开始就学习推理\n\n摘要：大型语言模型（LLMs）已经展示了令人印象深刻的实际应用能力，体现了人工实用智能（AUI）。然而，它们适应性和鲁棒性推理的能力——人工通用智能（AGI）的标志——仍然脆弱。尽管LLMs在常识推理、编程和数学方面似乎取得了成功，但它们在新颖情境中推广算法理解的能力仍然有限。我们在深奥编程语言中的算法任务实验表明，LLM的推理过度拟合训练数据，其可迁移性有限。我们假设这种有限可迁移性的核心问题是LLMs中推理和知识的耦合。  \n为了从AUI过渡到AGI，我们提出了通过三个关键方向来解耦知识和推理：（1）使用从头开始的强化学习（RL）进行推理预训练，作为广泛使用的下一个词预测预训练的替代方案，（2）使用合成任务的课程来简化RL推理先验的学习，然后将其迁移到自然语言任务中，（3）使用小上下文窗口学习更具通用性的推理函数，以减少利用词元之间的虚假相关性。这种推理系统与训练有素的检索系统和大型外部记忆库作为知识存储相结合，可以克服现有架构在新颖场景中学习推理的若干局限性。",
    "url": "https://huggingface.co/papers/2502.19402",
    "arxiv_url": "https://arxiv.org/abs/2502.19402"
  },
  {
    "title": "Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain\n  Model",
    "summary": "Room layout estimation from multiple-perspective images is poorly\ninvestigated due to the complexities that emerge from multi-view geometry,\nwhich requires muti-step solutions such as camera intrinsic and extrinsic\nestimation, image matching, and triangulation. However, in 3D reconstruction,\nthe advancement of recent 3D foundation models such as DUSt3R has shifted the\nparadigm from the traditional multi-step structure-from-motion process to an\nend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a\nnovel method for multi-view room layout estimation leveraging the 3D foundation\nmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on\na room layout dataset (Structure3D) with a modified objective to estimate\nstructural planes. By generating uniform and parsimonious results, Plane-DUSt3R\nenables room layout estimation with only a single post-processing step and 2D\ndetection results. Unlike previous methods that rely on single-perspective or\npanorama image, Plane-DUSt3R extends the setting to handle multiple-perspective\nimages. Moreover, it offers a streamlined, end-to-end solution that simplifies\nthe process and reduces error accumulation. Experimental results demonstrate\nthat Plane-DUSt3R not only outperforms state-of-the-art methods on the\nsynthetic dataset but also proves robust and effective on in the wild data with\ndifferent image styles such as cartoon.Our code is available at:\nhttps://github.com/justacar/Plane-DUSt3R",
    "translation": "标题：预训练模型时代下的非固定稀疏视角房间布局重建\n\n摘要：由于多视角几何带来的复杂性，从多视角图像中进行房间布局估计的研究较少，这需要多步骤的解决方案，如相机内外参数估计、图像匹配和三角测量。然而，在3D重建领域，最近的3D基础模型（如DUSt3R）的进展已经将传统的多步骤运动结构过程转变为端到端的单步骤方法。为此，我们提出了Plane-DUSt3R，这是一种利用3D基础模型DUSt3R进行多视角房间布局估计的新方法。Plane-DUSt3R结合了DUSt3R框架，并在房间布局数据集（Structure3D）上进行了微调，以估计结构平面。通过生成均匀且简洁的结果，Plane-DUSt3R仅需一个后处理步骤和2D检测结果即可实现房间布局估计。与以往依赖单视角或全景图像的方法不同，Plane-DUSt3R扩展了设置以处理多视角图像。此外，它提供了一个简化的端到端解决方案，简化了过程并减少了误差累积。实验结果表明，Plane-DUSt3R不仅在合成数据集上优于最先进的方法，而且在具有不同图像风格（如卡通）的真实数据上也表现出鲁棒性和有效性。我们的代码可在以下网址获取：https://github.com/justacar/Plane-DUSt3R",
    "url": "https://huggingface.co/papers/2502.16779",
    "arxiv_url": "https://arxiv.org/abs/2502.16779"
  },
  {
    "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security\n  Analysis",
    "summary": "Recent advancements in Web AI agents have demonstrated remarkable\ncapabilities in addressing complex web navigation tasks. However, emerging\nresearch shows that these agents exhibit greater vulnerability compared to\nstandalone Large Language Models (LLMs), despite both being built upon the same\nsafety-aligned models. This discrepancy is particularly concerning given the\ngreater flexibility of Web AI Agent compared to standalone LLMs, which may\nexpose them to a wider range of adversarial user inputs. To build a scaffold\nthat addresses these concerns, this study investigates the underlying factors\nthat contribute to the increased vulnerability of Web AI agents. Notably, this\ndisparity stems from the multifaceted differences between Web AI agents and\nstandalone LLMs, as well as the complex signals - nuances that simple\nevaluation metrics, such as success rate, often fail to capture. To tackle\nthese challenges, we propose a component-level analysis and a more granular,\nsystematic evaluation framework. Through this fine-grained investigation, we\nidentify three critical factors that amplify the vulnerability of Web AI\nagents; (1) embedding user goals into the system prompt, (2) multi-step action\ngeneration, and (3) observational capabilities. Our findings highlights the\npressing need to enhance security and robustness in AI agent design and provide\nactionable insights for targeted defense strategies.",
    "translation": "标题：为什么Web AI代理比独立的大型语言模型更脆弱？一项安全性分析\n\n摘要：近年来，Web AI代理在处理复杂的网络导航任务方面展示了显著的能力。然而，新兴研究表明，尽管这些代理与独立的大型语言模型（LLMs）都基于相同的安全对齐模型，但Web AI代理表现出更大的脆弱性。这种差异尤其令人担忧，因为与独立的LLMs相比，Web AI代理具有更大的灵活性，这可能使它们暴露于更广泛的对抗性用户输入中。为了解决这些问题，本研究探讨了导致Web AI代理脆弱性增加的潜在因素。值得注意的是，这种差异源于Web AI代理与独立LLMs之间的多方面差异，以及复杂的信号——这些细微差别往往是简单的评估指标（如成功率）无法捕捉的。为了应对这些挑战，我们提出了组件级分析和更细粒度、系统化的评估框架。通过这种细粒度的调查，我们识别出三个加剧Web AI代理脆弱性的关键因素：（1）将用户目标嵌入系统提示中，（2）多步骤动作生成，以及（3）观察能力。我们的研究结果强调了在AI代理设计中增强安全性和鲁棒性的迫切需求，并为有针对性的防御策略提供了可操作的见解。",
    "url": "https://huggingface.co/papers/2502.20383",
    "arxiv_url": "https://arxiv.org/abs/2502.20383"
  },
  {
    "title": "Direct Discriminative Optimization: Your Likelihood-Based Visual\n  Generative Model is Secretly a GAN Discriminator",
    "summary": "While likelihood-based generative models, particularly diffusion and\nautoregressive models, have achieved remarkable fidelity in visual generation,\nthe maximum likelihood estimation (MLE) objective inherently suffers from a\nmode-covering tendency that limits the generation quality under limited model\ncapacity. In this work, we propose Direct Discriminative Optimization (DDO) as\na unified framework that bridges likelihood-based generative training and the\nGAN objective to bypass this fundamental constraint. Our key insight is to\nparameterize a discriminator implicitly using the likelihood ratio between a\nlearnable target model and a fixed reference model, drawing parallels with the\nphilosophy of Direct Preference Optimization (DPO). Unlike GANs, this\nparameterization eliminates the need for joint training of generator and\ndiscriminator networks, allowing for direct, efficient, and effective\nfinetuning of a well-trained model to its full potential beyond the limits of\nMLE. DDO can be performed iteratively in a self-play manner for progressive\nmodel refinement, with each round requiring less than 1% of pretraining epochs.\nOur experiments demonstrate the effectiveness of DDO by significantly advancing\nthe previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to\nnew records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently\nimproving both guidance-free and CFG-enhanced FIDs of visual autoregressive\nmodels on ImageNet 256times256.",
    "translation": "标题：直接判别优化：基于似然的视觉生成模型实为GAN判别器\n\n摘要：尽管基于似然的生成模型，特别是扩散模型和自回归模型，在视觉生成方面取得了显著的保真度，但最大似然估计（MLE）目标本质上存在一种模式覆盖倾向，限制了在有限模型容量下的生成质量。在本研究中，我们提出了直接判别优化（DDO）作为一个统一框架，将基于似然的生成训练与GAN目标相结合，以绕过这一基本限制。我们的关键见解是使用可学习目标模型与固定参考模型之间的似然比来隐式参数化判别器，这与直接偏好优化（DPO）的理念相呼应。与GAN不同，这种参数化消除了生成器和判别器网络联合训练的需求，允许直接、高效且有效地微调训练良好的模型，使其超越MLE的限制发挥全部潜力。DDO可以以自我对抗的方式迭代进行，逐步优化模型，每轮所需的预训练轮次不到1%。我们的实验证明了DDO的有效性，显著提升了之前的最先进扩散模型EDM，将CIFAR-10/ImageNet-64数据集上的FID分数从1.79/1.58降低到新的记录1.30/0.97，并持续改善了ImageNet 256×256上视觉自回归模型的无引导和CFG增强的FID分数。",
    "url": "https://huggingface.co/papers/2503.01103",
    "arxiv_url": "https://arxiv.org/abs/2503.01103"
  },
  {
    "title": "AI-Invented Tonal Languages: Preventing a Machine Lingua Franca Beyond\n  Human Understanding",
    "summary": "This paper investigates the potential for large language models (LLMs) to\ndevelop private tonal languages for machine-to-machine (M2M) communication.\nInspired by cryptophasia in human twins (affecting up to 50% of twin births)\nand natural tonal languages like Mandarin and Vietnamese, we implement a\nprecise character-to-frequency mapping system that encodes the full ASCII\ncharacter set (32-126) using musical semitones. Each character is assigned a\nunique frequency, creating a logarithmic progression beginning with space (220\nHz) and ending with tilde (50,175.42 Hz). This spans approximately 7.9 octaves,\nwith higher characters deliberately mapped to ultrasonic frequencies beyond\nhuman perception (>20 kHz). Our implemented software prototype demonstrates\nthis encoding through visualization, auditory playback, and ABC musical\nnotation, allowing for analysis of information density and transmission speed.\nTesting reveals that tonal encoding can achieve information rates exceeding\nhuman speech while operating partially outside human perceptual boundaries.\nThis work responds directly to concerns about AI systems catastrophically\ndeveloping private languages within the next five years, providing a concrete\nprototype software example of how such communication might function and the\ntechnical foundation required for its emergence, detection, and governance.",
    "translation": "标题：AI发明的声调语言：防止超越人类理解的机器通用语\n\n摘要：本文探讨了大型语言模型（LLMs）为机器间（M2M）通信开发私有声调语言的潜力。受人类双胞胎中的隐秘语言现象（影响高达50%的双胞胎出生）以及汉语和越南语等自然声调语言的启发，我们实现了一个精确的字符到频率映射系统，该系统使用音乐半音编码完整的ASCII字符集（32-126）。每个字符被分配一个独特的频率，形成一个从空格（220 Hz）开始到波浪号（50,175.42 Hz）结束的对数级数。这大约跨越了7.9个八度，较高字符被有意映射到超出人类感知范围的超声波频率（>20 kHz）。我们实现的软件原型通过可视化、听觉播放和ABC音乐符号展示了这种编码，允许分析信息密度和传输速度。测试表明，声调编码可以在部分超出人类感知边界的情况下实现超过人类语音的信息速率。这项工作直接回应了关于AI系统在未来五年内灾难性地开发私密语言的担忧，提供了一个具体的原型软件示例，展示了这种通信可能如何运作，以及其出现、检测和治理所需的技术基础。",
    "url": "https://huggingface.co/papers/2503.01063",
    "arxiv_url": "https://arxiv.org/abs/2503.01063"
  },
  {
    "title": "CLEA: Closed-Loop Embodied Agent for Enhancing Task Execution in Dynamic\n  Environments",
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities in the\nhierarchical decomposition of complex tasks through semantic reasoning.\nHowever, their application in embodied systems faces challenges in ensuring\nreliable execution of subtask sequences and achieving one-shot success in\nlong-term task completion. To address these limitations in dynamic\nenvironments, we propose Closed-Loop Embodied Agent (CLEA) -- a novel\narchitecture incorporating four specialized open-source LLMs with functional\ndecoupling for closed-loop task management. The framework features two core\ninnovations: (1) Interactive task planner that dynamically generates executable\nsubtasks based on the environmental memory, and (2) Multimodal execution critic\nemploying an evaluation framework to conduct a probabilistic assessment of\naction feasibility, triggering hierarchical re-planning mechanisms when\nenvironmental perturbations exceed preset thresholds. To validate CLEA's\neffectiveness, we conduct experiments in a real environment with manipulable\nobjects, using two heterogeneous robots for object search, manipulation, and\nsearch-manipulation integration tasks. Across 12 task trials, CLEA outperforms\nthe baseline model, achieving a 67.3% improvement in success rate and a 52.8%\nincrease in task completion rate. These results demonstrate that CLEA\nsignificantly enhances the robustness of task planning and execution in dynamic\nenvironments.",
    "translation": "标题：CLEA：用于增强动态环境中任务执行的闭环具身代理\n\n摘要：大型语言模型（LLMs）通过语义推理在复杂任务的分层分解中展现出显著的能力。然而，它们在具身系统中的应用面临着确保子任务序列可靠执行和实现长期任务一次性成功的挑战。为了解决这些在动态环境中的局限性，我们提出了闭环具身代理（CLEA）——一种新颖的架构，结合了四个专门的开源LLMs，并采用功能解耦进行闭环任务管理。该框架具有两个核心创新：（1）交互式任务规划器，基于环境记忆动态生成可执行的子任务；（2）多模态执行批评器，采用评估框架对动作可行性进行概率评估，当环境扰动超过预设阈值时触发分层重新规划机制。为了验证CLEA的有效性，我们在一个具有可操作物体的真实环境中进行了实验，使用两个异构机器人进行物体搜索、操作以及搜索-操作集成任务。在12次任务试验中，CLEA优于基线模型，成功率提高了67.3%，任务完成率提高了52.8%。这些结果表明，CLEA显著增强了动态环境中任务规划和执行的鲁棒性。",
    "url": "https://huggingface.co/papers/2503.00729",
    "arxiv_url": "https://arxiv.org/abs/2503.00729"
  }
]